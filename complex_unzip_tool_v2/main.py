"""
Complex Unzip Tool v2 - Advanced zip file management
å¤æ‚è§£å‹å·¥å…· v2 - é«˜çº§å‹ç¼©æ–‡ä»¶ç®¡ç†å·¥å…·

A sop                    i          safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶") else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")rename_errors:
            for error in rename_errors:
                safe_print(error)
    else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")
    
    if verbose:     safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶") else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")     for error in rename_errors:
                safe_print(error)
    else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")f rename_errors:
            for error in rename_errors:
                safe_print(error)
    else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")f rename_errors:
            for error in rename_errors:
                safe_print(error)
    else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")       safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶") else:
        safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")       safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")       safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")       safe_safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")icated bilingual tool for analyzing and grouping archive files.
"""

import argparse
import shutil
import sys
import os
import re
from pathlib import Path
from typing import List, Optional

# Fix Windows console encoding for Unicode characters
if sys.platform.startswith('win'):
    try:
        # Set UTF-8 code page for Windows console
        os.system('chcp 65001 >nul 2>&1')
    except Exception:
        pass

from .file_collector import collect_all_files
from .file_grouper import group_files_by_priority
from .display_utils import display_file_groups
from .path_validator import validate_paths
from .password_manager import load_password_book, display_password_info, save_new_passwords
from .file_renamer import detect_cloaked_files, rename_cloaked_files, display_rename_suggestions
from .console_utils import safe_print
from .archive_extractor import (
    ExtractionResult, find_main_archive_in_group, extract_with_7z, 
    extract_nested_archives, create_completed_structure, clean_up_original_files,
    prompt_for_password, display_extraction_results, is_partial_archive,
    extract_partial_archive_and_reassemble, is_archive_file, get_ascii_temp_path,
    check_multipart_completeness, find_missing_parts_in_other_archives,
    extract_multipart_with_7z
)
from .multipart_detector import check_archive_completeness, prioritize_extraction_order
from .file_organizer import organize_and_cleanup, create_extraction_summary


import re
import shutil


def process_subfolder_group(group_name: str, group_files: List[Path], completed_dir: Path, 
                           extraction_result: ExtractionResult, processed_archives: set, 
                           passwords: List[str], root_path: Path, 
                           containers_to_cleanup: Optional[set] = None, extracted_parts_to_cleanup: Optional[set] = None) -> bool:
    """Process all files in a subfolder as a coordinated group.
    
    This function handles all files in a subfolder together, looking for multi-part archives
    and container files that may contain missing parts.
    
    Args:
        group_name: Name of the group
        group_files: List of files in the subfolder
        completed_dir: Directory for completed extractions (not used for subfolder groups)
        extraction_result: Result tracking object
        processed_archives: Set of already processed archives
        passwords: List of passwords to try
        root_path: Root path being processed
        
    Returns:
        True if any extraction was successful
    """
    # Initialize tracking sets if not provided
    if containers_to_cleanup is None:
        containers_to_cleanup = set()
    if extracted_parts_to_cleanup is None:
        extracted_parts_to_cleanup = set()
        
    safe_print(f"  ğŸ“ Processing subfolder group with {len(group_files)} files | å¤„ç†åŒ…å« {len(group_files)} ä¸ªæ–‡ä»¶çš„å­æ–‡ä»¶å¤¹ç»„")
    
    # Analyze files in the subfolder to identify multi-part archives and containers
    multipart_archives = {}  # base_name -> list of part files
    container_files = []  # potential container files like 11111.7z
    other_files = []
    
    # Group by archive patterns
    for file_path in group_files:
        file_name = file_path.name
        
        # Check for multi-part archive pattern (*.001, *.002, etc.)
        multipart_match = re.match(r'^(.+?)\.(\d{3})$', file_name, re.IGNORECASE)
        if multipart_match:
            base_name = multipart_match.group(1)
            part_num = int(multipart_match.group(2))
            
            if base_name not in multipart_archives:
                multipart_archives[base_name] = []
            multipart_archives[base_name].append((part_num, file_path))
        else:
            # Check if it might be a container file (common name patterns)
            if any(pattern in file_name.lower() for pattern in ['11111', 'container', 'parts']):
                container_files.append(file_path)
            else:
                other_files.append(file_path)
    
    total_extracted = 0
    successful_extractions = []
    
    # Process multi-part archives first
    for base_name, parts_list in multipart_archives.items():
        parts_list.sort()  # Sort by part number
        part_files = [path for _, path in parts_list]
        part_numbers = [num for num, _ in parts_list]
        
        safe_print(f"  ğŸ§© Found multi-part archive: {base_name} with parts {part_numbers} | å‘ç°å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶: {base_name}ï¼Œéƒ¨åˆ† {part_numbers}")
        
        # Helper function for part number extraction
        def extract_part_number(path):
            match = re.search(r'\.(\d{3})$', path.name)
            return int(match.group(1)) if match else 999
        
        # Use proper completeness checking function
        is_complete, found_parts, missing_parts = check_multipart_completeness(part_files, base_name)
        
        if not is_complete:
            safe_print(f"  âš ï¸ Multi-part archive incomplete! Found parts: {found_parts}, Missing: {missing_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ä¸å®Œæ•´ï¼æ‰¾åˆ°éƒ¨åˆ†: {found_parts}ï¼Œç¼ºå°‘: {missing_parts}")
            
            # First, search for missing parts in the same group (same subfolder)
            safe_print(f"  ğŸ” Searching for missing parts in same subfolder | åœ¨åŒä¸€å­æ–‡ä»¶å¤¹ä¸­æœç´¢ç¼ºå°‘çš„éƒ¨åˆ†")
            
            # Look for missing parts in container files within the same subfolder
            for missing_part in missing_parts[:]:  # Use slice to allow modification during iteration
                missing_file_name = f"{base_name}.{missing_part:03d}"
                
                # Check if the missing part might be in a container file
                for container_file in container_files:
                    if container_file in processed_archives:
                        continue
                        
                    safe_print(f"  ğŸ“¦ Checking container file for missing part {missing_part:03d}: {container_file.name}")
                    
                    # Extract container to current path (same directory as the container)
                    container_extract_dir = container_file.parent
                    
                    try:
                        success, message, password_used = extract_with_7z(container_file, container_extract_dir, passwords)
                        
                        if success:
                            # Track successfully processed archive
                            extraction_result.successfully_processed_archives.append(container_file)
                            processed_archives.add(container_file)
                            
                            safe_print(f"  âœ… Extracted container successfully | å®¹å™¨è§£å‹æˆåŠŸ")
                            
                            # Look for the missing part in the extracted content
                            potential_part_path = container_extract_dir / missing_file_name
                            if potential_part_path.exists():
                                safe_print(f"  ğŸ“„ Found missing part: {missing_file_name} | æ‰¾åˆ°ç¼ºå°‘çš„éƒ¨åˆ†: {missing_file_name}")
                                part_files.append(potential_part_path)
                                part_numbers.append(missing_part)
                                # Track this extracted part for cleanup
                                extracted_parts_to_cleanup.add(potential_part_path)
                                missing_parts.remove(missing_part)
                                break
                            else:
                                # Search in any subdirectories that might have been created
                                for extracted_file in container_extract_dir.rglob(missing_file_name):
                                    if extracted_file.is_file():
                                        # Move the file to the correct location
                                        dest_path = container_extract_dir / missing_file_name
                                        if extracted_file != dest_path:
                                            shutil.move(str(extracted_file), str(dest_path))
                                        safe_print(f"  ğŸ“„ Found and moved missing part: {missing_file_name}")
                                        part_files.append(dest_path)
                                        part_numbers.append(missing_part)
                                        # Track this extracted part for cleanup
                                        extracted_parts_to_cleanup.add(dest_path)
                                        missing_parts.remove(missing_part)
                                        break
                            
                            # Mark container as processed
                            processed_archives.add(container_file)
                            
                            if password_used:
                                extraction_result.passwords_used.append(password_used)
                        else:
                            safe_print(f"  âŒ Failed to extract container: {message}")
                            
                    except Exception as e:
                        safe_print(f"  âŒ Error extracting container: {e}")
            
            # Re-check completeness after searching in containers
            # Create a list of all files (original + found parts) to check completeness
            all_available_files = part_files.copy()
            
            # Scan the directory for any new parts that might have been extracted
            if part_files:
                current_dir = part_files[0].parent
                for file_path in current_dir.iterdir():
                    if file_path.is_file() and base_name.lower() in file_path.name.lower() and re.search(r'\.\d{3}$', file_path.suffix):
                        if file_path not in all_available_files:
                            all_available_files.append(file_path)
            
            is_complete, found_parts, missing_parts = check_multipart_completeness(all_available_files, base_name)
            
            if is_complete:
                safe_print(f"  âœ… Multi-part archive is now complete with parts: {found_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ç°åœ¨å®Œæ•´ï¼ŒåŒ…å«éƒ¨åˆ†: {found_parts}")
                part_files = all_available_files  # Update part_files with all found parts
        else:
            safe_print(f"  âœ… Multi-part archive is complete with parts: {found_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶å®Œæ•´ï¼ŒåŒ…å«éƒ¨åˆ†: {found_parts}")
        
        # Now try to extract the multi-part archive only if complete
        # Now try to extract the multi-part archive only if complete
        if not is_complete:
            safe_print(f"  âš ï¸ Still missing parts: {missing_parts}, cannot extract | ä»ç¼ºå°‘éƒ¨åˆ†: {missing_parts}ï¼Œæ— æ³•è§£å‹")
            extraction_result.failed_extractions.append((f"{group_name}_{base_name}", f"Missing parts: {missing_parts}"))
        else:
            # We have all parts, try extraction
            part_files.sort(key=extract_part_number)
            if part_files and part_files[0].name.endswith('.001'):
                try:
                    # Extract to current path (same directory as the parts)
                    current_extract_dir = part_files[0].parent
                    success, message, password_used = extract_multipart_with_7z(part_files, current_extract_dir, passwords)
                    
                    if success:
                        safe_print(f"  âœ… Successfully extracted multi-part archive: {base_name} | æˆåŠŸè§£å‹å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶: {base_name}")
                        total_extracted += 1
                        successful_extractions.append(f"{base_name}: multi-part archive")
                        
                        # Check if extracted content contains partial archive files
                        extracted_files = list(current_extract_dir.rglob('*'))
                        extracted_files = [f for f in extracted_files if f.is_file() and f.parent != current_extract_dir]  # Exclude original archive files
                        
                        contains_partial_archives = any(
                            re.search(r'\.\d{3}$', f.name) or 'part' in f.name.lower() 
                            for f in extracted_files
                        )
                        
                        if not contains_partial_archives and extracted_files:
                            # Move extracted content to _Unzipped folder
                            safe_print(f"  ğŸ“ Moving final extracted content to _Unzipped folder | å°†æœ€ç»ˆè§£å‹å†…å®¹ç§»åŠ¨åˆ° _Unzipped æ–‡ä»¶å¤¹")
                            unzipped_dir = root_path / "_Unzipped"
                            unzipped_dir.mkdir(parents=True, exist_ok=True)
                            
                            moved_count = 0
                            for extracted_file in extracted_files:
                                try:
                                    # Calculate relative path from extraction directory, skipping intermediate folders
                                    rel_path = extracted_file.relative_to(current_extract_dir)
                                    
                                    # Skip the first part if it looks like an archive name
                                    if len(rel_path.parts) > 1:
                                        first_part = rel_path.parts[0]
                                        # If the first part looks like an archive name, skip it
                                        if any(ext in first_part.lower() for ext in ['.7z', '.zip', '.rar']) or first_part.endswith('.7z'):
                                            dest_rel_path = Path(*rel_path.parts[1:]) if len(rel_path.parts) > 1 else Path(rel_path.name)
                                        else:
                                            dest_rel_path = rel_path
                                    else:
                                        dest_rel_path = rel_path
                                    
                                    dest_path = unzipped_dir / dest_rel_path
                                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                                    shutil.move(str(extracted_file), str(dest_path))
                                    moved_count += 1
                                except Exception as e:
                                    safe_print(f"  âš ï¸ Failed to move {extracted_file.name}: {e}")
                            
                            safe_print(f"  âœ… Moved {moved_count} files to _Unzipped | ç§»åŠ¨äº† {moved_count} ä¸ªæ–‡ä»¶åˆ° _Unzipped")
                        
                        # Mark all parts as processed
                        for part_file in part_files:
                            processed_archives.add(part_file)
                            
                        if password_used:
                            extraction_result.passwords_used.append(password_used)
                    else:
                        safe_print(f"  âŒ Failed to extract multi-part archive: {message}")
                        extraction_result.failed_extractions.append((f"{group_name}_{base_name}", message))
                        
                except Exception as e:
                    safe_print(f"  âŒ Error extracting multi-part archive: {e}")
                    extraction_result.failed_extractions.append((f"{group_name}_{base_name}", str(e)))
    
    # Process remaining individual files (excluding already processed containers)
    for file_path in other_files + container_files:
        if file_path in processed_archives:
            continue
            
        if is_archive_file(file_path):
            try:
                # Extract to current path (same directory as the archive)
                current_extract_dir = file_path.parent
                success, message, password_used = extract_with_7z(file_path, current_extract_dir, passwords)
                
                if success:
                    # Track successfully processed archive
                    extraction_result.successfully_processed_archives.append(file_path)
                    
                    safe_print(f"  âœ… Successfully extracted individual archive: {file_path.name}")
                    total_extracted += 1
                    successful_extractions.append(f"{file_path.name}: individual archive")
                    processed_archives.add(file_path)
                    
                    # Check if extracted content contains partial archive files
                    extracted_files = list(current_extract_dir.rglob('*'))
                    extracted_files = [f for f in extracted_files if f.is_file() and f.parent != current_extract_dir]  # Exclude original archive files
                    
                    contains_partial_archives = any(
                        re.search(r'\.\d{3}$', f.name) or 'part' in f.name.lower() 
                        for f in extracted_files
                    )
                    
                    if not contains_partial_archives and extracted_files:
                        # Move extracted content to _Unzipped folder
                        safe_print(f"  ğŸ“ Moving final extracted content to _Unzipped folder | å°†æœ€ç»ˆè§£å‹å†…å®¹ç§»åŠ¨åˆ° _Unzipped æ–‡ä»¶å¤¹")
                        unzipped_dir = root_path / "_Unzipped" / group_name / file_path.stem
                        unzipped_dir.mkdir(parents=True, exist_ok=True)
                        
                        moved_count = 0
                        for extracted_file in extracted_files:
                            try:
                                # Calculate relative path from extraction directory
                                rel_path = extracted_file.relative_to(current_extract_dir)
                                dest_path = unzipped_dir / rel_path
                                dest_path.parent.mkdir(parents=True, exist_ok=True)
                                shutil.move(str(extracted_file), str(dest_path))
                                moved_count += 1
                            except Exception as e:
                                safe_print(f"  âš ï¸ Failed to move {extracted_file.name}: {e}")
                        
                        safe_print(f"  âœ… Moved {moved_count} files to _Unzipped | ç§»åŠ¨äº† {moved_count} ä¸ªæ–‡ä»¶åˆ° _Unzipped")
                    
                    if password_used:
                        extraction_result.passwords_used.append(password_used)
                else:
                    safe_print(f"  âŒ Failed to extract individual archive: {message}")
                    extraction_result.failed_extractions.append((f"{group_name}_{file_path.name}", message))
                    
            except Exception as e:
                safe_print(f"  âŒ Error extracting individual archive: {e}")
                extraction_result.failed_extractions.append((f"{group_name}_{file_path.name}", str(e)))
    
    if successful_extractions:
        extraction_result.successful_extractions.append((group_name, f"{total_extracted} files extracted from {len(successful_extractions)} archives"))
        safe_print(f"  âœ… Subfolder group completed: {total_extracted} extractions | å­æ–‡ä»¶å¤¹ç»„å®Œæˆ: {total_extracted} ä¸ªè§£å‹")
        
        # Clean up original files after successful extraction
        safe_print(f"  ğŸ—‘ï¸ Cleaning up original files | æ¸…ç†åŸå§‹æ–‡ä»¶")
        cleaned_count = 0
        failed_count = 0
        
        for file_path in group_files:
            if file_path in processed_archives:
                try:
                    if file_path.exists():
                        file_path.unlink()
                        cleaned_count += 1
                        safe_print(f"    ğŸ—‘ï¸ Deleted: {file_path.name}")
                except Exception as e:
                    failed_count += 1
                    safe_print(f"    âŒ Failed to delete {file_path.name}: {e}")
        
        # Debug: Show what we're tracking for cleanup
        if containers_to_cleanup or extracted_parts_to_cleanup:
            safe_print(f"  ğŸ” Debug: Containers to cleanup: {len(containers_to_cleanup)} | å®¹å™¨æ¸…ç†è¿½è¸ª: {len(containers_to_cleanup)}")
            safe_print(f"  ğŸ” Debug: Extracted parts to cleanup: {len(extracted_parts_to_cleanup)} | è§£å‹éƒ¨åˆ†æ¸…ç†è¿½è¸ª: {len(extracted_parts_to_cleanup)}")
        
        # Clean up container archives that were used for this group
        containers_deleted = 0
        containers_failed = 0
        for container_archive in containers_to_cleanup:
            try:
                if container_archive.exists():
                    container_archive.unlink()
                    containers_deleted += 1
                    safe_print(f"    ğŸ—‘ï¸ Cleaned up container: {container_archive.name} | æ¸…ç†å®¹å™¨: {container_archive.name}")
            except Exception as e:
                containers_failed += 1
                safe_print(f"    âŒ Failed to clean up container {container_archive.name}: {e} | æ¸…ç†å®¹å™¨å¤±è´¥ {container_archive.name}: {e}")
        
        # Clean up extracted parts that were found in containers
        extracted_parts_deleted = 0
        extracted_parts_failed = 0
        for extracted_part in extracted_parts_to_cleanup:
            try:
                if extracted_part.exists():
                    extracted_part.unlink()
                    extracted_parts_deleted += 1
                    safe_print(f"    ğŸ—‘ï¸ Cleaned up extracted part: {extracted_part.name} | æ¸…ç†è§£å‹çš„éƒ¨åˆ†: {extracted_part.name}")
            except Exception as e:
                extracted_parts_failed += 1
                safe_print(f"    âŒ Failed to clean up extracted part {extracted_part.name}: {e} | æ¸…ç†è§£å‹éƒ¨åˆ†å¤±è´¥ {extracted_part.name}: {e}")
        
        # Clean up extracted content folders and temp files in the subfolder
        subfolder_cleanup_deleted = 0
        subfolder_cleanup_failed = 0
        
        # Get the subfolder path
        subfolder_path = group_files[0].parent if group_files else None
        if subfolder_path and subfolder_path.exists():
            # Clean up temp multipart files
            for temp_file in subfolder_path.glob("temp_multipart_*"):
                try:
                    if temp_file.is_file():
                        temp_file.unlink()
                        subfolder_cleanup_deleted += 1
                        safe_print(f"    ğŸ—‘ï¸ Cleaned up temp file: {temp_file.name} | æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file.name}")
                except Exception as e:
                    subfolder_cleanup_failed += 1
                    safe_print(f"    âŒ Failed to clean up temp file {temp_file.name}: {e} | æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file.name}: {e}")
            
            # Clean up remaining archive parts that weren't processed
            for archive_file in subfolder_path.glob("*.7z.*"):
                try:
                    if archive_file.is_file() and re.search(r'\.\d{3}$', archive_file.suffix):
                        archive_file.unlink()
                        subfolder_cleanup_deleted += 1
                        safe_print(f"    ğŸ—‘ï¸ Cleaned up unused archive part: {archive_file.name} | æ¸…ç†æœªä½¿ç”¨çš„å‹ç¼©æ–‡ä»¶éƒ¨åˆ†: {archive_file.name}")
                except Exception as e:
                    subfolder_cleanup_failed += 1
                    safe_print(f"    âŒ Failed to clean up archive part {archive_file.name}: {e} | æ¸…ç†å‹ç¼©æ–‡ä»¶éƒ¨åˆ†å¤±è´¥ {archive_file.name}: {e}")
            
            # Clean up extracted content folders (folders that were created by extraction)
            for item in subfolder_path.iterdir():
                if item.is_dir() and not item.name.startswith('.'):
                    # Check if this is likely an extracted content folder
                    # Since we've already moved content to _Unzipped, we can remove extracted folders
                    try:
                        # Remove any folder that looks like extracted content or is empty
                        item_files = list(item.rglob('*'))
                        item_files = [f for f in item_files if f.is_file()]
                        
                        # Remove if it's an extracted content folder (has many files) or is empty
                        should_remove = (
                            len(item_files) == 0 or  # Empty folder
                            len(item_files) > 5 or   # Likely extracted content
                            any(keyword in item.name.lower() for keyword in ['xiuren', 'ç§€äºº', 'é±¼å­é…±', 'p+', 'g]', 'ã€', 'ã€‘'])  # Content folder patterns
                        )
                        
                        if should_remove:
                            shutil.rmtree(item, ignore_errors=True)
                            if not item.exists():
                                subfolder_cleanup_deleted += 1
                                safe_print(f"    ğŸ—‘ï¸ Cleaned up extracted folder: {item.name} | æ¸…ç†è§£å‹æ–‡ä»¶å¤¹: {item.name}")
                            else:
                                subfolder_cleanup_failed += 1
                                safe_print(f"    âš ï¸ Failed to fully clean up folder: {item.name} | æ— æ³•å®Œå…¨æ¸…ç†æ–‡ä»¶å¤¹: {item.name}")
                    except Exception as e:
                        subfolder_cleanup_failed += 1
                        safe_print(f"    âŒ Failed to clean up folder {item.name}: {e} | æ¸…ç†æ–‡ä»¶å¤¹å¤±è´¥ {item.name}: {e}")
        
        # Update totals
        cleaned_count += containers_deleted + extracted_parts_deleted + subfolder_cleanup_deleted
        failed_count += containers_failed + extracted_parts_failed + subfolder_cleanup_failed
        
        # Try to remove the subfolder if it's empty
        subfolder_path = group_files[0].parent if group_files else None
        if subfolder_path and subfolder_path.exists():
            try:
                remaining_files = list(subfolder_path.rglob('*'))
                remaining_files = [f for f in remaining_files if f.is_file()]
                
                if not remaining_files:
                    subfolder_path.rmdir()
                    safe_print(f"  ğŸ—‘ï¸ Deleted empty subfolder: {subfolder_path.name} | åˆ é™¤ç©ºå­æ–‡ä»¶å¤¹: {subfolder_path.name}")
                else:
                    safe_print(f"  âš ï¸ Subfolder {subfolder_path.name} still contains {len(remaining_files)} files - keeping | å­æ–‡ä»¶å¤¹ {subfolder_path.name} ä»åŒ…å« {len(remaining_files)} ä¸ªæ–‡ä»¶ - ä¿ç•™")
            except Exception as e:
                safe_print(f"  âŒ Failed to delete subfolder {subfolder_path.name}: {e}")
        
        safe_print(f"  ğŸ—‘ï¸ Cleanup completed: {cleaned_count} files deleted, {failed_count} failed | æ¸…ç†å®Œæˆ: {cleaned_count} ä¸ªæ–‡ä»¶å·²åˆ é™¤ï¼Œ{failed_count} ä¸ªå¤±è´¥")
        return True
    else:
        safe_print(f"  âŒ No successful extractions in subfolder group | å­æ–‡ä»¶å¤¹ç»„ä¸­æ— æˆåŠŸè§£å‹")
        return False


def process_paths(paths: List[Path], recursive: bool = False, verbose: bool = False, 
                 dry_run: bool = False, no_extract: bool = False) -> None:
    """Process the provided paths for unzipping operations.
    
    Args:
        paths: List of validated Path objects (files or directories)
        recursive: Whether to recursively search directories
        verbose: Whether to show detailed information
        dry_run: Whether to simulate renaming without actually doing it
        no_extract: Whether to skip archive extraction (extraction is default)
    """
    safe_print(f"Complex Unzip Tool v2 - Processing {len(paths)} path(s)")
    safe_print(f"å¤æ‚è§£å‹å·¥å…· v2 - æ­£åœ¨å¤„ç† {len(paths)} ä¸ªè·¯å¾„")
    
    # Determine root path from the first path
    root_path = paths[0].parent if paths[0].is_file() else paths[0]
    
    # Collect all files from paths
    all_files = collect_all_files(paths, recursive)
    
    # Detect and rename cloaked files
    cloaked_files = detect_cloaked_files(all_files)
    if cloaked_files:
        safe_print(f"\nğŸ” Detected {len(cloaked_files)} cloaked files | æ£€æµ‹åˆ° {len(cloaked_files)} ä¸ªä¼ªè£…æ–‡ä»¶")
        if verbose:
            display_rename_suggestions(cloaked_files)
        
        # Automatically rename cloaked files
        successful_renames, rename_errors = rename_cloaked_files(cloaked_files, dry_run=False)
        if rename_errors:
            safe_print("âš ï¸ Some files could not be renamed | éƒ¨åˆ†æ–‡ä»¶æ— æ³•é‡å‘½å:")
            for error in rename_errors:
                safe_print(f"  âŒ {error}")
        elif successful_renames:
            safe_print("âœ… All cloaked files renamed successfully | æ‰€æœ‰ä¼ªè£…æ–‡ä»¶é‡å‘½åæˆåŠŸ")
        
        # Re-collect files after renaming
        all_files = collect_all_files(paths, recursive)
    else:
        safe_print("âœ“ No cloaked files detected | æœªæ£€æµ‹åˆ°ä¼ªè£…æ–‡ä»¶")
    
    # Load password book and determine save location
    passwords = load_password_book(root_path)
    
    # Determine best location for saving new passwords
    possible_save_locations = [
        Path.cwd() / "passwords.txt",  # Current working directory (project dir) - preferred
        root_path / "passwords.txt",  # Target directory
    ]
    
    passwords_file = None
    for location in possible_save_locations:
        try:
            # Test if we can write to this location
            if location.exists() or location.parent.exists():
                passwords_file = location
                break
        except Exception:
            continue
    
    if not passwords_file:
        passwords_file = Path.cwd() / "passwords.txt"  # Fallback to current directory
    
    if verbose:
        safe_print("\nğŸ“‹ All files found | æ‰¾åˆ°çš„æ‰€æœ‰æ–‡ä»¶:")
        for file_path in sorted(all_files):
            safe_print(f"   ğŸ“„ {file_path}")
        
        # Display password information if verbose
        display_password_info(passwords, verbose=True)
    
    # Group files with root-aware priority logic
    priority_groups = group_files_by_priority(all_files, root_path)
    
    # Display the groups
    display_file_groups(priority_groups, verbose)
    
    # Perform extraction by default (unless disabled or dry-run)
    if not no_extract and not dry_run:
        safe_print("\n" + "=" * 60)
        safe_print("ğŸš€ STARTING ARCHIVE EXTRACTION | å¼€å§‹å‹ç¼©æ–‡ä»¶è§£å‹")
        safe_print("=" * 60)
        
        # Step 1: Check multi-part archive completeness
        multipart_archives, potential_missing_containers = check_archive_completeness(all_files, verbose)
        
        extraction_result = ExtractionResult()
        
        # Track archives that have been processed to avoid duplicate processing
        processed_archives = set()
        
        # Track container archives that should be cleaned up after successful extraction
        containers_to_cleanup = set()
        
        # Track extracted parts that should be cleaned up after successful extraction
        extracted_parts_to_cleanup = set()
        
        # Track successfully extracted files for organization
        extracted_files = []
        
        # Track successfully processed archive files for cleanup
        successfully_processed_archives = []
        
        # Add back variables needed by existing code
        completed_dir = root_path / "_Unzipped"
        passwords_file = root_path / "passwords.txt"
        processed_subfolders = set()
        
        # Step 2: Prioritize extraction order based on multi-part analysis
        if multipart_archives or potential_missing_containers:
            # Get files from all groups for prioritization
            all_group_files = []
            for group_files in priority_groups.values():
                all_group_files.extend(group_files)
            
            # Prioritize extraction order
            prioritized_files = prioritize_extraction_order(multipart_archives, all_group_files)
            
            safe_print(f"\nğŸ“‹ Extraction order optimized based on multi-part analysis | åŸºäºå¤šéƒ¨åˆ†åˆ†æä¼˜åŒ–è§£å‹é¡ºåº")
            if verbose and prioritized_files:
                safe_print("ğŸ”„ Optimized extraction order:")
                for i, file_path in enumerate(prioritized_files[:10], 1):  # Show first 10
                    safe_print(f"   {i}. {file_path.name}")
                if len(prioritized_files) > 10:
                    safe_print(f"   ... and {len(prioritized_files) - 10} more files")
        else:
            # Use original group-based processing
            prioritized_files = []
            for group_files in priority_groups.values():
                prioritized_files.extend(group_files)
        
        # Process each group in priority_groups, but prioritize groups with partial archives first
        groups_to_process = list(priority_groups.items())
        
        safe_print(f"\nğŸ” Analyzing groups for partial archive priority... | åˆ†æç»„ä»¥ç¡®å®šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ä¼˜å…ˆçº§...")
        
        # Sort groups to prioritize those containing partial archives
        def group_priority(group_item):
            group_name, group_files = group_item
            # Check if any file in this group might contain partial archive content
            for file_path in group_files:
                if is_archive_file(file_path):
                    # Quick heuristic checks first
                    file_name = file_path.name.lower()
                    
                    # If filename suggests it might be a single part of something, prioritize it
                    if any(indicator in file_name for indicator in ['11111', 'part', 'vol', 'disc']):
                        safe_print(f"  ğŸ§© Priority: {group_name} might contain partial content: {file_path.name} | ä¼˜å…ˆçº§: {group_name} å¯èƒ½åŒ…å«éƒ¨åˆ†å†…å®¹: {file_path.name}")
                        return 0
                    
                    # Try actual detection with very short timeout
                    try:
                        # Use ASCII temp path for the detection to avoid encoding issues
                        temp_file, needs_cleanup = get_ascii_temp_path(file_path)
                        try:
                            is_partial, base_name = is_partial_archive(temp_file)
                            if is_partial:
                                safe_print(f"  ğŸ§© Priority: {group_name} contains partial archive: {file_path.name} | ä¼˜å…ˆçº§: {group_name} åŒ…å«éƒ¨åˆ†å‹ç¼©æ–‡ä»¶: {file_path.name}")
                                return 0  # High priority for partial archives
                        finally:
                            if needs_cleanup and temp_file.exists():
                                try:
                                    temp_file.unlink()
                                except Exception:
                                    pass
                    except Exception as e:
                        safe_print(f"  âš ï¸ Error checking {file_path.name}: {e} | æ£€æŸ¥æ—¶å‡ºé”™ {file_path.name}: {e}")
                        # If we can't check, but the name suggests partial content, prioritize anyway
                        if any(indicator in file_name for indicator in ['11111', 'part', 'vol']):
                            return 0
                        continue
            return 1  # Lower priority for regular archives
        
        groups_to_process.sort(key=group_priority)
        
        for group_name, group_files in groups_to_process:
            safe_print(f"\nğŸ“¦ Processing group: {group_name} | å¤„ç†ç»„: {group_name}")
            safe_print("-" * 40)
            
            # Check if this is a subfolder group (all files in same subfolder)
            if "_subfolder_all" in group_name:
                # This is a subfolder group - process all files together
                if process_subfolder_group(group_name, group_files, completed_dir, extraction_result, processed_archives, passwords, root_path, containers_to_cleanup, extracted_parts_to_cleanup):
                    # Subfolder group processed successfully
                    continue
                else:
                    # Subfolder group processing failed, but continue with other groups
                    continue
            
            # Find the main archive to extract
            main_archive = find_main_archive_in_group(group_files)
            
            if not main_archive:
                safe_print(f"  âŒ No main archive found in group | ç»„ä¸­æœªæ‰¾åˆ°ä¸»å‹ç¼©æ–‡ä»¶")
                extraction_result.failed_extractions.append((group_name, "No main archive found"))
                continue
            
            # Check if this archive was already processed as a container
            if main_archive in processed_archives:
                safe_print(f"  â­ï¸ Archive already processed as container, skipping: {main_archive.name} | å‹ç¼©æ–‡ä»¶å·²ä½œä¸ºå®¹å™¨å¤„ç†ï¼Œè·³è¿‡: {main_archive.name}")
                continue
            
            safe_print(f"  ğŸ¯ Main archive: {main_archive.name} | ä¸»å‹ç¼©æ–‡ä»¶: {main_archive.name}")
            
            # Check if this is a multi-part archive and if it's complete
            is_multipart = False
            is_complete = False
            base_name = ""
            if re.match(r'.*\.001$', main_archive.name, re.IGNORECASE):
                is_multipart = True
                base_name = re.sub(r'\.001$', '', main_archive.name, flags=re.IGNORECASE)
                
                safe_print(f"  ğŸ§© Detected multi-part archive: {base_name} | æ£€æµ‹åˆ°å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶: {base_name}")
                
                # Check completeness by scanning the actual directory for all parts
                # This is important because other groups might have extracted additional parts
                archive_dir = main_archive.parent
                all_parts_in_dir = []
                for file_path in archive_dir.iterdir():
                    if file_path.is_file() and base_name.lower() in file_path.name.lower() and re.search(r'\.\d{3}$', file_path.suffix):
                        all_parts_in_dir.append(file_path)
                
                safe_print(f"  ğŸ” Scanning directory for all parts of {base_name} | æ‰«æç›®å½•ä¸­ {base_name} çš„æ‰€æœ‰éƒ¨åˆ†")
                for part_file in all_parts_in_dir:
                    safe_print(f"     Found part: {part_file.name} | æ‰¾åˆ°éƒ¨åˆ†: {part_file.name}")
                
                # Check completeness using all parts found in directory
                is_complete, found_parts, missing_parts = check_multipart_completeness(all_parts_in_dir, base_name)
                
                if not is_complete:
                    safe_print(f"  âš ï¸ Multi-part archive incomplete! Found parts: {found_parts}, Missing: {missing_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ä¸å®Œæ•´ï¼æ‰¾åˆ°éƒ¨åˆ†: {found_parts}ï¼Œç¼ºå°‘: {missing_parts}")
                    
                    # First, look for missing parts in the same group/path
                    safe_print(f"  ğŸ” Searching for missing parts in same group first | é¦–å…ˆåœ¨åŒç»„ä¸­æœç´¢ç¼ºå°‘çš„éƒ¨åˆ†")
                    found_in_group = {}
                    
                    for missing_part in missing_parts:
                        expected_name = f"{base_name}.{missing_part:03d}"
                        # Look in the current group files first
                        for group_file in group_files:
                            if group_file.name.lower() == expected_name.lower():
                                found_in_group[missing_part] = group_file
                                safe_print(f"    ğŸ“„ Found missing part {missing_part:03d} in same group: {group_file.name}")
                                break
                    
                    # Remove parts found in the same group from missing list
                    for found_part in found_in_group.keys():
                        if found_part in missing_parts:
                            missing_parts.remove(found_part)
                    
                    # Update found_parts list
                    found_parts.extend(found_in_group.keys())
                    
                    # If still missing parts, look for them in other archives
                    if missing_parts:
                        safe_print(f"  ğŸ” Still missing parts: {missing_parts}, searching in other archives | ä»ç¼ºå°‘éƒ¨åˆ†: {missing_parts}ï¼Œåœ¨å…¶ä»–å‹ç¼©æ–‡ä»¶ä¸­æœç´¢")
                        
                        # Look for missing parts in other archives
                        part_locations = find_missing_parts_in_other_archives(missing_parts, base_name, priority_groups)
                        
                        if part_locations:
                            safe_print(f"  ğŸ” Found missing parts in other archives: | åœ¨å…¶ä»–å‹ç¼©æ–‡ä»¶ä¸­æ‰¾åˆ°ç¼ºå°‘çš„éƒ¨åˆ†:")
                            for part_num, container_archive in part_locations.items():
                                safe_print(f"     Part {part_num:03d} found in: {container_archive.name} | éƒ¨åˆ† {part_num:03d} åœ¨æ­¤æ‰¾åˆ°: {container_archive.name}")
                            
                            # Extract the containers to current path to get the missing parts
                            extracted_any_parts = False
                            for part_num, container_archive in part_locations.items():
                                safe_print(f"  ğŸ“¦ Extracting container for part {part_num:03d}: {container_archive.name} | ä¸ºéƒ¨åˆ† {part_num:03d} è§£å‹å®¹å™¨: {container_archive.name}")
                                
                                # Extract container to current path (same directory as main archive)
                                container_extract_dir = main_archive.parent
                                
                                try:
                                    # For container extraction, extract to current path
                                    success, message, password_used = extract_with_7z(
                                        container_archive, container_extract_dir, passwords
                                    )
                                    
                                    if success:
                                        # Track successfully processed archive
                                        extraction_result.successfully_processed_archives.append(container_archive)
                                        
                                        safe_print(f"  âœ… Extracted container successfully to current path | å®¹å™¨æˆåŠŸè§£å‹åˆ°å½“å‰è·¯å¾„")
                                        extracted_any_parts = True
                                        
                                        # Mark this archive as processed to avoid processing it again later
                                        processed_archives.add(container_archive)
                                        
                                        # Mark this container for cleanup when main extraction succeeds
                                        containers_to_cleanup.add(container_archive)
                                        
                                        if password_used and password_used not in passwords:
                                            passwords.append(password_used)
                                            extraction_result.new_passwords.append(password_used)
                                        
                                        # The missing parts should now be available in the current directory
                                        expected_part_name = f"{base_name}.{part_num:03d}"
                                        expected_part_path = main_archive.parent / expected_part_name
                                        
                                        if expected_part_path.exists():
                                            safe_print(f"  ğŸ“„ Found extracted missing part: {expected_part_name} | æ‰¾åˆ°è§£å‹çš„ç¼ºå°‘éƒ¨åˆ†: {expected_part_name}")
                                            # Track this extracted part for cleanup
                                            extracted_parts_to_cleanup.add(expected_part_path)
                                        else:
                                            safe_print(f"  âš ï¸ Missing part {expected_part_name} not found after extraction | è§£å‹åæœªæ‰¾åˆ°ç¼ºå°‘çš„éƒ¨åˆ† {expected_part_name}")
                                    else:
                                        safe_print(f"  âŒ Failed to extract container: {message} | è§£å‹å®¹å™¨å¤±è´¥: {message}")
                                        
                                except Exception as e:
                                    safe_print(f"  âŒ Error extracting container: {e} | è§£å‹å®¹å™¨æ—¶å‡ºé”™: {e}")
                            
                            if not extracted_any_parts:
                                safe_print(f"  âŒ Failed to extract any missing parts from containers | ä»å®¹å™¨ä¸­è§£å‹ä»»ä½•ç¼ºå°‘éƒ¨åˆ†å¤±è´¥")
                                extraction_result.failed_extractions.append((group_name, "Failed to extract missing parts from containers"))
                                continue
                        else:
                            safe_print(f"  âŒ Missing parts not found in any other archives | åœ¨ä»»ä½•å…¶ä»–å‹ç¼©æ–‡ä»¶ä¸­éƒ½æœªæ‰¾åˆ°ç¼ºå°‘çš„éƒ¨åˆ†")
                            extraction_result.failed_extractions.append((group_name, f"Multi-part archive incomplete. Missing parts: {missing_parts}"))
                            continue
                    
                    # Re-check completeness after finding parts in group or extracting from containers
                    if found_in_group or (missing_parts and 'part_locations' in locals() and part_locations):
                        safe_print(f"  ğŸ”„ Re-checking completeness after finding missing parts... | æ‰¾åˆ°ç¼ºå°‘éƒ¨åˆ†åé‡æ–°æ£€æŸ¥å®Œæ•´æ€§...")
                        
                        # Rescan the directory to find all parts (including newly found ones)
                        archive_dir = main_archive.parent
                        all_parts_in_dir = []
                        for file_path in archive_dir.iterdir():
                            if file_path.is_file() and base_name.lower() in file_path.name.lower() and re.search(r'\.\d{3}$', file_path.suffix):
                                all_parts_in_dir.append(file_path)
                        
                        is_complete, found_parts, missing_parts = check_multipart_completeness(all_parts_in_dir, base_name)
                        
                        if is_complete:
                            safe_print(f"  âœ… Multi-part archive is now complete with parts: {found_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ç°åœ¨å®Œæ•´ï¼ŒåŒ…å«éƒ¨åˆ†: {found_parts}")
                            # Update the group files to include the newly found parts
                            group_files = all_parts_in_dir
                        else:
                            safe_print(f"  âš ï¸ Multi-part archive still incomplete. Found parts: {found_parts}, Missing: {missing_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ä»ä¸å®Œæ•´ã€‚æ‰¾åˆ°éƒ¨åˆ†: {found_parts}ï¼Œç¼ºå°‘: {missing_parts}")
                            extraction_result.failed_extractions.append((group_name, f"Multi-part archive incomplete. Missing parts: {missing_parts}"))
                            continue
                        
                    else:
                        safe_print(f"  âŒ Missing parts not found in any other archives | åœ¨ä»»ä½•å…¶ä»–å‹ç¼©æ–‡ä»¶ä¸­éƒ½æœªæ‰¾åˆ°ç¼ºå°‘çš„éƒ¨åˆ†")
                        extraction_result.failed_extractions.append((group_name, f"Multi-part archive incomplete. Missing parts: {missing_parts}"))
                        continue
                else:
                    safe_print(f"  âœ… Multi-part archive is complete with parts: {found_parts} | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶å®Œæ•´ï¼ŒåŒ…å«éƒ¨åˆ†: {found_parts}")
                    # Update the group files to include all parts found in directory
                    group_files = all_parts_in_dir
            
            # Create temporary extraction directory
            temp_extract_dir = main_archive.parent / f"temp_extract_{group_name}"
            
            try:
                # Check if this is a partial archive that needs special handling
                # But skip partial archive logic if this is a complete multi-part archive
                is_partial, base_name_partial = is_partial_archive(main_archive)
                
                if is_partial and not (is_multipart and is_complete):
                    # This is a partial archive (not a complete multi-part archive)
                    safe_print(f"  ğŸ§© Detected partial archive content, extracting and reassembling... | æ£€æµ‹åˆ°éƒ¨åˆ†å‹ç¼©æ–‡ä»¶å†…å®¹ï¼Œæ­£åœ¨è§£å‹å’Œé‡æ–°ç»„è£…...")
                    success, message, password_used = extract_partial_archive_and_reassemble(main_archive, temp_extract_dir, passwords)
                else:
                    # Regular archive extraction (including complete multi-part archives)
                    if is_multipart and is_complete:
                        # For multi-part archives, we need to handle all parts together
                        safe_print(f"  ğŸ§© Multi-part archive detected, handling all {len(group_files)} parts together | æ£€æµ‹åˆ°å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶ï¼Œæ­£åœ¨å¤„ç†æ‰€æœ‰ {len(group_files)} ä¸ªéƒ¨åˆ†")
                        success, message, password_used = extract_multipart_with_7z(group_files, temp_extract_dir, passwords)
                    else:
                        # Single archive extraction
                        success, message, password_used = extract_with_7z(main_archive, temp_extract_dir, passwords)
                
                # If extraction failed, prompt for password
                if not success and "password" in message.lower():
                    user_password = prompt_for_password(main_archive.name)
                    if user_password:
                        passwords.append(user_password)
                        if is_partial and not (is_multipart and is_complete):
                            success, message, password_used = extract_partial_archive_and_reassemble(main_archive, temp_extract_dir, [user_password])
                        else:
                            success, message, password_used = extract_with_7z(main_archive, temp_extract_dir, [user_password])
                        if success:
                            extraction_result.new_passwords.append(user_password)
                
                if success:
                    # Track successfully processed archive
                    extraction_result.successfully_processed_archives.append(main_archive)
                    
                    safe_print(f"  âœ… Extracted main archive successfully | ä¸»å‹ç¼©æ–‡ä»¶è§£å‹æˆåŠŸ")
                    
                    # Extract nested archives recursively
                    safe_print(f"  ğŸ”„ Checking for nested archives... | æ£€æŸ¥åµŒå¥—å‹ç¼©æ–‡ä»¶...")
                    
                    final_files, new_passwords = extract_nested_archives(temp_extract_dir, passwords)
                    extraction_result.new_passwords.extend(new_passwords)
                    
                    # Determine extraction destination based on content
                    # If the archive contains partial archive files, extract to current path
                    contains_partial_archives = any(
                        re.search(r'\.\d{3}$', f.name) or 'part' in f.name.lower() 
                        for f in final_files if f.is_file()
                    )
                    
                    if contains_partial_archives:
                        safe_print(f"  ğŸ“¦ Archive contains partial files, extracting to current path | å‹ç¼©æ–‡ä»¶åŒ…å«éƒ¨åˆ†æ–‡ä»¶ï¼Œè§£å‹åˆ°å½“å‰è·¯å¾„")
                        # Extract to current path (same directory as the archive)
                        current_extract_dir = main_archive.parent
                        
                        # Copy files directly to the current directory, preserving structure
                        copied_count = 0
                        for file_path in final_files:
                            if file_path.is_file():
                                # Calculate relative path from temp extraction directory
                                try:
                                    rel_path = file_path.relative_to(temp_extract_dir)
                                    dest_path = current_extract_dir / rel_path
                                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                                    shutil.copy2(file_path, dest_path)
                                    copied_count += 1
                                except Exception as e:
                                    safe_print(f"  âš ï¸ Failed to copy {file_path.name}: {e}")
                        
                        safe_print(f"  âœ… Extracted {copied_count} files to current path | è§£å‹ {copied_count} ä¸ªæ–‡ä»¶åˆ°å½“å‰è·¯å¾„")
                        extraction_result.completed_files.extend([current_extract_dir / f.relative_to(temp_extract_dir) 
                                                                for f in final_files if f.is_file()])
                    else:
                        # Regular extraction to _Unzipped directory
                        safe_print(f"  ğŸ“ Moving final extracted content to _Unzipped folder | å°†æœ€ç»ˆè§£å‹å†…å®¹ç§»åŠ¨åˆ° _Unzipped æ–‡ä»¶å¤¹")
                        
                        # For subfolder groups, move content directly to _Unzipped without group name nesting
                        moved_count = 0
                        
                        # Find the actual content and move it directly to _Unzipped
                        for file_path in final_files:
                            if file_path.is_file():
                                try:
                                    # Get the relative path from temp extraction directory
                                    rel_path = file_path.relative_to(temp_extract_dir)
                                    
                                    # For nested structure like: archive_name/content_folder/files
                                    # We want to extract: content_folder/files directly to _Unzipped
                                    # Skip the archive name part if it exists
                                    if len(rel_path.parts) > 1:
                                        # Check if the first part is the archive name (common pattern)
                                        first_part = rel_path.parts[0]
                                        # If the first part looks like an archive name, skip it
                                        if any(ext in first_part.lower() for ext in ['.7z', '.zip', '.rar']) or first_part.endswith('.7z'):
                                            dest_rel_path = Path(*rel_path.parts[1:]) if len(rel_path.parts) > 1 else Path(rel_path.name)
                                        else:
                                            dest_rel_path = rel_path
                                    else:
                                        # Direct file
                                        dest_rel_path = rel_path
                                    
                                    dest_path = completed_dir / dest_rel_path
                                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                                    shutil.copy2(file_path, dest_path)
                                    moved_count += 1
                                except Exception as e:
                                    safe_print(f"  âš ï¸ Failed to move {file_path.name}: {e}")
                        
                        safe_print(f"  âœ… Moved {moved_count} files to _Unzipped | ç§»åŠ¨äº† {moved_count} ä¸ªæ–‡ä»¶åˆ° _Unzipped")
                        # Update the tracking to reflect the new locations
                        extraction_result.completed_files.extend([completed_dir / f.relative_to(temp_extract_dir) 
                                                                for f in final_files if f.is_file()])
                    
                    # Clean up temporary directory
                    if temp_extract_dir.exists():
                        shutil.rmtree(temp_extract_dir, ignore_errors=True)
                    
                    # Determine if extraction was successful based on whether files were moved/copied
                    extraction_successful = False
                    extracted_file_count = 0
                    
                    if contains_partial_archives:
                        # For partial archives (extracted to current path), check copied_count
                        if 'copied_count' in locals() and copied_count > 0:
                            extraction_successful = True
                            extracted_file_count = copied_count
                    else:
                        # For regular archives (moved to _Unzipped), check moved_count
                        if 'moved_count' in locals() and moved_count > 0:
                            extraction_successful = True
                            extracted_file_count = moved_count
                    
                    if extraction_successful:
                        # Only clean up original files if we have successfully extracted content
                        safe_print(f"  âœ… Valid extraction completed, cleaning up original files | æœ‰æ•ˆè§£å‹å®Œæˆï¼Œæ¸…ç†åŸå§‹æ–‡ä»¶")
                        
                        # Clean up original files
                        deleted, failed = clean_up_original_files(group_files)
                        
                        # Debug: Show what we're tracking for cleanup
                        safe_print(f"  ğŸ” Debug: Containers to cleanup: {len(containers_to_cleanup)} | å®¹å™¨æ¸…ç†è¿½è¸ª: {len(containers_to_cleanup)}")
                        safe_print(f"  ğŸ” Debug: Extracted parts to cleanup: {len(extracted_parts_to_cleanup)} | è§£å‹éƒ¨åˆ†æ¸…ç†è¿½è¸ª: {len(extracted_parts_to_cleanup)}")
                        
                        # Also clean up container archives that were used for this group
                        containers_deleted = 0
                        containers_failed = 0
                        for container_archive in containers_to_cleanup:
                            try:
                                if container_archive.exists():
                                    container_archive.unlink()
                                    containers_deleted += 1
                                    safe_print(f"  ğŸ—‘ï¸  Cleaned up container: {container_archive.name} | æ¸…ç†å®¹å™¨: {container_archive.name}")
                            except Exception as e:
                                containers_failed += 1
                                safe_print(f"  âŒ Failed to clean up container {container_archive.name}: {e} | æ¸…ç†å®¹å™¨å¤±è´¥ {container_archive.name}: {e}")
                        
                        # Also clean up extracted parts that were found in containers
                        extracted_parts_deleted = 0
                        extracted_parts_failed = 0
                        for extracted_part in extracted_parts_to_cleanup:
                            try:
                                if extracted_part.exists():
                                    extracted_part.unlink()
                                    extracted_parts_deleted += 1
                                    safe_print(f"  ğŸ—‘ï¸  Cleaned up extracted part: {extracted_part.name} | æ¸…ç†è§£å‹çš„éƒ¨åˆ†: {extracted_part.name}")
                            except Exception as e:
                                extracted_parts_failed += 1
                                safe_print(f"  âŒ Failed to clean up extracted part {extracted_part.name}: {e} | æ¸…ç†è§£å‹éƒ¨åˆ†å¤±è´¥ {extracted_part.name}: {e}")
                        
                        # Clean up extracted content folders and temp files in the subfolder
                        subfolder_cleanup_deleted = 0
                        subfolder_cleanup_failed = 0
                        
                        # Get the subfolder path
                        if group_files:
                            subfolder_path = group_files[0].parent
                            
                            # Clean up temp multipart files
                            for temp_file in subfolder_path.glob("temp_multipart_*"):
                                try:
                                    if temp_file.is_file():
                                        temp_file.unlink()
                                        subfolder_cleanup_deleted += 1
                                        safe_print(f"  ğŸ—‘ï¸  Cleaned up temp file: {temp_file.name} | æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file.name}")
                                except Exception as e:
                                    subfolder_cleanup_failed += 1
                                    safe_print(f"  âŒ Failed to clean up temp file {temp_file.name}: {e} | æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file.name}: {e}")
                            
                            # Clean up extracted content folders (folders that were created by extraction)
                            for item in subfolder_path.iterdir():
                                if item.is_dir() and not item.name.startswith('.'):
                                    # Check if this is likely an extracted content folder
                                    # (folders that contain many files and weren't there originally)
                                    try:
                                        item_files = list(item.rglob('*'))
                                        if len(item_files) > 10:  # Likely extracted content
                                            shutil.rmtree(item, ignore_errors=True)
                                            if not item.exists():
                                                subfolder_cleanup_deleted += 1
                                                safe_print(f"  ğŸ—‘ï¸  Cleaned up extracted folder: {item.name} | æ¸…ç†è§£å‹æ–‡ä»¶å¤¹: {item.name}")
                                            else:
                                                subfolder_cleanup_failed += 1
                                    except Exception as e:
                                        subfolder_cleanup_failed += 1
                                        safe_print(f"  âŒ Failed to clean up folder {item.name}: {e} | æ¸…ç†æ–‡ä»¶å¤¹å¤±è´¥ {item.name}: {e}")
                        
                        total_deleted = deleted + containers_deleted + extracted_parts_deleted + subfolder_cleanup_deleted
                        total_failed = failed + containers_failed + extracted_parts_failed + subfolder_cleanup_failed
                        safe_print(f"  ğŸ—‘ï¸  Cleaned up: {total_deleted} files deleted, {total_failed} failed | æ¸…ç†å®Œæˆ: {total_deleted} ä¸ªæ–‡ä»¶å·²åˆ é™¤ï¼Œ{total_failed} ä¸ªå¤±è´¥")
                        
                        extraction_result.successful_extractions.append((group_name, extracted_file_count))
                    else:
                        safe_print(f"  âŒ No valid files extracted (only temporary files found) | æœªè§£å‹å‡ºæœ‰æ•ˆæ–‡ä»¶ï¼ˆä»…æ‰¾åˆ°ä¸´æ—¶æ–‡ä»¶ï¼‰")
                        safe_print(f"  ğŸ”„ Preserving original files due to failed extraction | ç”±äºè§£å‹å¤±è´¥ä¿ç•™åŸå§‹æ–‡ä»¶")
                        extraction_result.failed_extractions.append((group_name, "No valid files extracted (only temporary files)"))
                        
                        # Only clean up temp files, but preserve original archive files
                        if group_files:
                            subfolder_path = group_files[0].parent
                            temp_cleanup_count = 0
                            
                            # Clean up only temp multipart files
                            for temp_file in subfolder_path.glob("temp_multipart_*"):
                                try:
                                    if temp_file.is_file():
                                        temp_file.unlink()
                                        temp_cleanup_count += 1
                                        safe_print(f"  ğŸ—‘ï¸  Cleaned up temp file: {temp_file.name} | æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file.name}")
                                except Exception as e:
                                    safe_print(f"  âŒ Failed to clean up temp file {temp_file.name}: {e} | æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file.name}: {e}")
                            
                            if temp_cleanup_count > 0:
                                safe_print(f"  ğŸ—‘ï¸  Cleaned up: {temp_cleanup_count} temp files, preserved original files | æ¸…ç†å®Œæˆ: {temp_cleanup_count} ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹æ–‡ä»¶")
                    
                    # Track subfolder for cleanup if this group represents a subfolder
                    if not group_name.startswith("root_"):
                        # Extract subfolder name from group name (format: {folder_name}_{subgroup_name})
                        subfolder_name = group_name.split('_')[0]
                        if subfolder_name:
                            processed_subfolders.add(subfolder_name)
                    
                else:
                    # Error message is already cleaned by the extraction function
                    safe_print(f"  âŒ Extraction failed: {message}")
                    extraction_result.failed_extractions.append((group_name, message))
                    
                    # Clean up failed extraction directory
                    if temp_extract_dir.exists():
                        shutil.rmtree(temp_extract_dir, ignore_errors=True)
                
            except Exception as e:
                safe_print(f"  âŒ Error processing group: {e} | å¤„ç†ç»„æ—¶å‡ºé”™: {e}")
                extraction_result.failed_extractions.append((group_name, str(e)))
                
                # Clean up on error
                if temp_extract_dir.exists():
                    shutil.rmtree(temp_extract_dir, ignore_errors=True)
        
        # Save new passwords to password book
        if extraction_result.new_passwords:
            save_new_passwords(passwords_file, extraction_result.new_passwords)
        
        # Clean up successfully processed subfolders
        if processed_subfolders:
            safe_print(f"\nğŸ—‚ï¸  Cleaning up successfully processed subfolders... | æ¸…ç†æˆåŠŸå¤„ç†çš„å­æ–‡ä»¶å¤¹...")
            subfolder_deleted = 0
            subfolder_failed = 0
            
            for subfolder_name in processed_subfolders:
                subfolder_path = root_path / subfolder_name
                if subfolder_path.exists() and subfolder_path.is_dir():
                    try:
                        # Check if subfolder is empty or only contains files we've processed
                        remaining_files = list(subfolder_path.rglob('*'))
                        remaining_files = [f for f in remaining_files if f.is_file()]
                        
                        if not remaining_files:
                            # Subfolder is empty, safe to delete
                            shutil.rmtree(subfolder_path)
                            subfolder_deleted += 1
                            safe_print(f"  ğŸ—‘ï¸  Deleted empty subfolder: {subfolder_name} | åˆ é™¤ç©ºå­æ–‡ä»¶å¤¹: {subfolder_name}")
                        else:
                            safe_print(f"  âš ï¸  Subfolder {subfolder_name} still contains {len(remaining_files)} files - skipping deletion | å­æ–‡ä»¶å¤¹ {subfolder_name} ä»åŒ…å« {len(remaining_files)} ä¸ªæ–‡ä»¶ - è·³è¿‡åˆ é™¤")
                    except Exception as e:
                        subfolder_failed += 1
                        safe_print(f"  âŒ Failed to delete subfolder {subfolder_name}: {e} | åˆ é™¤å­æ–‡ä»¶å¤¹å¤±è´¥ {subfolder_name}: {e}")
            
            if subfolder_deleted > 0 or subfolder_failed > 0:
                safe_print(f"  ğŸ—‚ï¸  Subfolder cleanup: {subfolder_deleted} deleted, {subfolder_failed} failed | å­æ–‡ä»¶å¤¹æ¸…ç†: {subfolder_deleted} ä¸ªå·²åˆ é™¤ï¼Œ{subfolder_failed} ä¸ªå¤±è´¥")
        
        # Display final results
        display_extraction_results(extraction_result)
        
        # Step 3: Organize extracted files and cleanup archives
        if extraction_result.successful_extractions:
            safe_print("\n" + "=" * 60)
            safe_print("ğŸ“ ORGANIZING AND CLEANING UP | æ•´ç†å’Œæ¸…ç†")
            safe_print("=" * 60)
            
            # Use the completed_files from extraction_result
            all_extracted_files = extraction_result.completed_files
            
            # Get successfully processed archives for cleanup
            archive_files_to_cleanup = extraction_result.successfully_processed_archives.copy()
            
            # Add archives from the containers and parts tracking
            archive_files_to_cleanup.extend(containers_to_cleanup)
            archive_files_to_cleanup.extend(extracted_parts_to_cleanup)
            
            # Also add archives from processed_archives set
            archive_files_to_cleanup.extend(processed_archives)
            
            # Remove duplicates and ensure they exist
            unique_archives = []
            seen = set()
            for archive_path in archive_files_to_cleanup:
                archive_path = Path(archive_path)  # Ensure it's a Path object
                if archive_path not in seen and archive_path.exists():
                    seen.add(archive_path)
                    unique_archives.append(archive_path)
            
            # Perform organization and cleanup
            success = organize_and_cleanup(
                extracted_files=all_extracted_files,
                archive_files=unique_archives,
                root_path=root_path,
                verbose=verbose
            )
            
            if success:
                # Create extraction summary
                unzipped_dir = root_path / "_Unzipped"
                moved_files = list(unzipped_dir.rglob('*')) if unzipped_dir.exists() else []
                moved_files = [f for f in moved_files if f.is_file()]
                
                create_extraction_summary(
                    moved_files=moved_files,
                    cleaned_archives=unique_archives,
                    unzipped_dir=unzipped_dir
                )
        else:
            safe_print("\nâš ï¸  No successful extractions to organize | æ²¡æœ‰æˆåŠŸçš„è§£å‹æ–‡ä»¶éœ€è¦æ•´ç†")
        
    elif not no_extract and dry_run:
        safe_print("\nğŸ’¡ Extraction not performed in dry-run mode | é¢„æ¼”æ¨¡å¼ä¸‹ä¸æ‰§è¡Œè§£å‹")
        safe_print("ğŸ’¡ Use without --dry-run to perform actual extraction | ä¸ä½¿ç”¨ --dry-run æ‰§è¡Œå®é™…è§£å‹")
    elif no_extract:
        safe_print("\nğŸ’¡ Extraction skipped (--no-extract specified) | è·³è¿‡è§£å‹ï¼ˆæŒ‡å®šäº† --no-extractï¼‰")
        
        # Still perform multi-part analysis for informational purposes
        safe_print("\n" + "=" * 60)
        safe_print("ğŸ“Š MULTI-PART ARCHIVE ANALYSIS | å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶åˆ†æ")
        safe_print("=" * 60)
        multipart_archives, potential_missing_containers = check_archive_completeness(all_files, verbose)
    
    # Display password book summary (non-verbose)
    if not verbose and passwords:
        display_password_info(passwords, verbose=False)
    
    safe_print("\n" + "-" * 50)
    safe_print("Processing complete! | å¤„ç†å®Œæˆï¼")


def main():
    """Main entry point for the application."""
    parser = argparse.ArgumentParser(
        description="Complex Unzip Tool v2 - Advanced archive extraction and management tool\n"
                   "å¤æ‚è§£å‹å·¥å…· v2 - é«˜çº§å‹ç¼©æ–‡ä»¶è§£å‹å’Œç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Features | åŠŸèƒ½:
  * Automatic archive extraction with 7z.exe (default) | ä½¿ç”¨7z.exeè‡ªåŠ¨è§£å‹ï¼ˆé»˜è®¤ï¼‰
  * Recursive directory processing (default) | é€’å½’ç›®å½•å¤„ç†ï¼ˆé»˜è®¤ï¼‰
  * Automatic cloaked file renaming (*.001åˆ  â†’ *.001) | è‡ªåŠ¨ä¼ªè£…æ–‡ä»¶é‡å‘½å
  * Password book loading from passwords.txt | ä»passwords.txtåŠ è½½å¯†ç æœ¬
  * Intelligent multi-part archive detection | æ™ºèƒ½å¤šéƒ¨åˆ†å‹ç¼©æ–‡ä»¶æ£€æµ‹
  * Recursive nested archive extraction | é€’å½’åµŒå¥—å‹ç¼©æ–‡ä»¶è§£å‹
  * Interactive password prompting | äº¤äº’å¼å¯†ç æç¤º
  * Automatic file organization to 'completed' folder | è‡ªåŠ¨æ–‡ä»¶æ•´ç†åˆ°'completed'æ–‡ä»¶å¤¹
  * Bilingual interface (English/Chinese) | åŒè¯­ç•Œé¢ï¼ˆè‹±æ–‡/ä¸­æ–‡ï¼‰

Examples | ç¤ºä¾‹:
  complex-unzip /path/to/archives                    # Extract all archives recursively (default) | é€’å½’è§£å‹æ‰€æœ‰å‹ç¼©æ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰
  complex-unzip --no-recursive /path/to/directory    # Only process files in the specified directory | ä»…å¤„ç†æŒ‡å®šç›®å½•ä¸­çš„æ–‡ä»¶
  complex-unzip --no-extract /path/to/directory      # Only analyze without extraction | ä»…åˆ†æä¸è§£å‹
  complex-unzip --dry-run /path/to/directory         # Preview renaming without extraction | é¢„è§ˆé‡å‘½åä½†ä¸è§£å‹
  complex-unzip --verbose /path/to/directory         # Show detailed extraction info | æ˜¾ç¤ºè¯¦ç»†è§£å‹ä¿¡æ¯
        """
    )
    
    parser.add_argument(
        "paths",
        nargs="*",
        help="One or more file paths or directory paths to process\n"
             "ä¸€ä¸ªæˆ–å¤šä¸ªè¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„"
    )
    
    parser.add_argument(
        "-r", "--no-recursive",
        action="store_true",
        help="Disable recursive directory search (recursive is default)\n"
             "ç¦ç”¨é€’å½’ç›®å½•æœç´¢ï¼ˆé»˜è®¤ä¸ºé€’å½’ï¼‰"
    )
    
    parser.add_argument(
        "-o", "--output",
        type=str,
        help="Output directory for extracted files (optional)\n"
             "è§£å‹æ–‡ä»¶çš„è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Enable verbose output for detailed information\n"
             "å¯ç”¨è¯¦ç»†è¾“å‡ºä»¥è·å–è¯¦ç»†ä¿¡æ¯"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate file renaming without actually renaming files (renaming is automatic)\n"
             "æ¨¡æ‹Ÿæ–‡ä»¶é‡å‘½åè€Œä¸å®é™…é‡å‘½åæ–‡ä»¶ï¼ˆé‡å‘½åæ˜¯è‡ªåŠ¨çš„ï¼‰"
    )
    
    parser.add_argument(
        "--no-extract",
        action="store_true",
        help="Skip archive extraction (extraction is performed by default)\n"
             "è·³è¿‡å‹ç¼©æ–‡ä»¶è§£å‹ï¼ˆé»˜è®¤æ‰§è¡Œè§£å‹ï¼‰"
    )
    
    args = parser.parse_args()
    
    # Handle drag and drop scenario (when used as compiled EXE)
    # If no paths provided, show help but also check for interactive mode
    if not args.paths:
        # Check if running as compiled EXE and suggest drag and drop
        import sys
        if getattr(sys, 'frozen', False):
            # Running as compiled EXE
            safe_print("ğŸ–±ï¸  Drag and Drop Mode | æ‹–æ‹½æ¨¡å¼")
            safe_print("=" * 50)
            safe_print("This tool supports drag and drop!")
            safe_print("æ­¤å·¥å…·æ”¯æŒæ‹–æ‹½æ“ä½œï¼")
            safe_print("")
            safe_print("To use:")
            safe_print("ä½¿ç”¨æ–¹æ³•ï¼š")
            safe_print("1. Drag files or folders onto this EXE file")
            safe_print("   å°†æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹æ‹–æ‹½åˆ°æ­¤ EXE æ–‡ä»¶ä¸Š")
            safe_print("2. Or run from command line with paths as arguments")
            safe_print("   æˆ–ä»å‘½ä»¤è¡Œè¿è¡Œå¹¶æä¾›è·¯å¾„å‚æ•°")
            safe_print("")
            safe_print("For command line usage:")
            safe_print("å‘½ä»¤è¡Œç”¨æ³•ï¼š")
            parser.print_help()
            safe_print("")
            input("Press Enter to exit | æŒ‰å›è½¦é”®é€€å‡º...")
        else:
            # Running in development mode
            parser.print_help()
        return
    
    # Show drag and drop confirmation for EXE mode
    import sys
    if getattr(sys, 'frozen', False) and len(args.paths) > 0:
        safe_print("ğŸ–±ï¸  Files/folders received via drag and drop!")
        safe_print("ğŸ–±ï¸  é€šè¿‡æ‹–æ‹½æ¥æ”¶åˆ°æ–‡ä»¶/æ–‡ä»¶å¤¹ï¼")
        safe_print("=" * 50)
        for i, path in enumerate(args.paths, 1):
            safe_print(f"{i}. {path}")
        safe_print("=" * 50)
        safe_print("")
    
    if args.verbose:
        safe_print("Verbose mode enabled | è¯¦ç»†æ¨¡å¼å·²å¯ç”¨")
        safe_print(f"Arguments | å‚æ•°: {vars(args)}")
    
    # Validate paths
    try:
        validated_paths = validate_paths(args.paths)
    except FileNotFoundError as e:
        safe_print(f"âŒ Error: {e}")
        exit(1)
    
    # Determine recursive behavior (default is True, disabled by --no-recursive or -r)
    recursive = not args.no_recursive
    
    # Process the paths
    process_paths(validated_paths, recursive, args.verbose, args.dry_run, args.no_extract)
    
    # Pause for EXE mode so users can see results
    import sys
    if getattr(sys, 'frozen', False):
        safe_print("")
        safe_print("=" * 50)
        input("Press Enter to exit | æŒ‰å›è½¦é”®é€€å‡º...")


if __name__ == "__main__":
    main()
